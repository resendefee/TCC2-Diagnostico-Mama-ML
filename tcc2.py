# -*- coding: utf-8 -*-
"""TCC2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FerSx0VME6bdHnW8GGBYDBRB2Lp8Lyms
"""

# TCC 2 - Aplicação de ML em dados de FNA (Câncer de mama)
# Base: UCI Machine Learning Repository - id=17
# Autora: Fernanda Ester Resende Moraes

# Importações básicas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib, os, sys, subprocess, warnings
from pathlib import Path

SEED = 11
np.random.seed(SEED)
warnings.filterwarnings("ignore")

# Instala pacotes apenas se faltarem
def instalar_se_necessario(pacote, nome_pip=None):
    nome_pip = nome_pip or pacote
    try:
        __import__(pacote)
    except Exception:
        print(f"Instalando dependência ausente: {nome_pip}")
        subprocess.check_call([sys.executable, "-m", "pip", "install", nome_pip, "-q"])

# Base WDBC direto da UCI com 'ucimlrepo' (fallback pelo scikit-learn)
instalar_se_necessario("ucimlrepo")

# Imports de ML (scikit-learn)
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay
)
from sklearn.calibration import calibration_curve
from sklearn.inspection import permutation_importance

# Modelos clássicos + MLP
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# Carrega a base Wisconsin Diagnostic Breast Cancer (WDBC, ID 17) diretamente do repositório UCI Machine Learning.
# Classe positiva = 1 (maligno), classe negativa = 0 (benigno).

from ucimlrepo import fetch_ucirepo

def carregar_wdbc_uci():
    wdbc = fetch_ucirepo(id=17)
    X = wdbc.data.features.copy()
    y_raw = wdbc.data.targets.copy()

    # Corrige eventuais diferenças de nome da coluna 'Diagnosis'
    col_diag = [c for c in y_raw.columns if c.lower() == "diagnosis"][0]
    y = y_raw[col_diag].map({"B": 0, "M": 1}).astype(int)

    return X, y

# Carregamento principal
print("Carregando dados WDBC (FNA) diretamente da UCI...")
X, y = carregar_wdbc_uci()
print(f"   Formato de X: {X.shape[0]} amostras x {X.shape[1]} atributos")
print("   Distribuição de y (1=Maligno, 0=Benigno):")
print(y.value_counts(normalize=True).rename({1: "Maligno", 0: "Benigno"}))

# Particionamento treino/teste — estratificado e reprodutível
X_treino, X_teste, y_treino, y_teste = train_test_split(
    X, y, test_size=0.20, random_state=SEED, stratify=y
)
print(f"Particionamento: treino={X_treino.shape}; teste={X_teste.shape}")

#Calcula a Especificidade (verdadeiros negativos sobre todos os negativos).
def especificidade(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp) if (tn + fp) > 0 else 0.0

# Definição de validação e dicionários de pipelines + grades de busca
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
pipelines = {}
grades = {}

# Regressão Logística (baseline linear)
pipelines["logreg"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", LogisticRegression(
        max_iter=500, random_state=SEED, solver="liblinear",
        class_weight="balanced"
    ))
])
grades["logreg"] = {
    "kbest__k": [10, 20, "all"],
    "clf__C": [0.1, 1, 10],
    "clf__penalty": ["l1", "l2"]
}

# SVM com kernel RBF
pipelines["svm_rbf"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", SVC(
        kernel="rbf", probability=True, random_state=SEED,
        class_weight="balanced"
    ))
])
grades["svm_rbf"] = {
    "kbest__k": [10, 20, "all"],
    "clf__C": [0.5, 1, 5],
    "clf__gamma": ["scale", 0.01]
}

# Random Forest
pipelines["rf"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", RandomForestClassifier(random_state=SEED))
])
grades["rf"] = {
    "kbest__k": [10, 20, "all"],
    "clf__n_estimators": [200, 400],
    "clf__max_depth": [None, 6, 10],
    "clf__min_samples_leaf": [1, 3]
}

# KNN
pipelines["knn"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", KNeighborsClassifier())
])
grades["knn"] = {
    "kbest__k": [10, 20, "all"],
    "clf__n_neighbors": [3, 5, 7, 9],
    "clf__weights": ["uniform", "distance"],
    "clf__p": [1, 2]  # distância de Manhattan (1) ou Euclidiana (2)
}

# Naive Bayes Gaussiano
pipelines["naive_bayes"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", GaussianNB())
])
grades["naive_bayes"] = {
    "kbest__k": [10, 20, "all"]
}

# MLP (ANN)
pipelines["ann_mlp"] = Pipeline([
    ("escalonador", StandardScaler()),
    ("kbest", SelectKBest(score_func=f_classif)),
    ("clf", MLPClassifier(random_state=SEED, max_iter=1000, early_stopping=True))
])
grades["ann_mlp"] = {
    "kbest__k": [10, 20, "all"],
    "clf__hidden_layer_sizes": [(50,), (100,), (50, 25)],
    "clf__alpha": [0.0001, 0.001]
}

# Funções para avaliação e gráficos
def avaliar_em_teste(modelo, X_test, y_test):
    # Calcula um conjunto de métricas sobre o conjunto de TESTE.
    # Retorna um dicionário com as principais métricas clínicas e estatísticas.
    y_prob = (modelo.predict_proba(X_test)[:, 1]
              if hasattr(modelo[-1], "predict_proba")
              else modelo.decision_function(X_test))
    y_pred = modelo.predict(X_test)

    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    resultados = {
        "test_roc_auc": roc_auc_score(y_test, y_prob),
        "test_pr_auc": average_precision_score(y_test, y_prob),
        "test_accuracy": accuracy_score(y_test, y_pred),
        "test_sensitivity": recall_score(y_test, y_pred, pos_label=1),  # sensibilidade do maligno
        "test_specificity": especificidade(y_test, y_pred),
        "test_f1": f1_score(y_test, y_pred, pos_label=1),
        "tn": tn, "fp": fp, "fn": fn, "tp": tp
    }
    return resultados, y_prob, y_pred

def plotar_graficos_desempenho(y_test, y_prob, y_pred, nome, pasta):
    # Plota e salva: Matriz de Confusão, Curva ROC, Curva Precisão-Recall e Curva de Calibração.
    pasta.mkdir(exist_ok=True)
    # Matriz de Confusão
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
    plt.title(f"Matriz de Confusão — {nome}")
    plt.grid(False)
    plt.savefig(pasta / f"{nome}_matriz_confusao.png", dpi=150, bbox_inches="tight")
    plt.show()

    # Curva ROC
    RocCurveDisplay.from_predictions(y_test, y_prob)
    plt.title(f"Curva ROC — {nome}")
    plt.grid(True)
    plt.savefig(pasta / f"{nome}_roc.png", dpi=150, bbox_inches="tight")
    plt.show()

    # Curva Precisão-Recall
    PrecisionRecallDisplay.from_predictions(y_test, y_prob)
    plt.title(f"Curva Precisão-Recall — {nome}")
    plt.grid(True)
    plt.savefig(pasta / f"{nome}_pr.png", dpi=150, bbox_inches="tight")
    plt.show()

    # Calibração (confiabilidade das probabilidades previstas)
    prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy="quantile")
    plt.figure()
    plt.plot(prob_pred, prob_true, marker="o", label="Modelo")
    plt.plot([0, 1], [0, 1], "--", label="Calibração Ideal")
    plt.title(f"Curva de Calibração — {nome}")
    plt.xlabel("Probabilidade prevista")
    plt.ylabel("Fração positiva real")
    plt.legend()
    plt.grid(True)
    plt.savefig(pasta / f"{nome}_calibracao.png", dpi=150, bbox_inches="tight")
    plt.show()

def interpretar_modelo(modelo, X_test, y_test, nome, pasta, colunas_X):
    # Tenta extrair interpretabilidade:
    # Se o classificador expõe importâncias (RandomForest) ou coeficientes (modelos lineares), plota o TOP 15.
    # Caso contrário, usa importância por permutação (modelo-agnóstica).
    print(f">> Interpretabilidade — {nome}")
    pasta.mkdir(exist_ok=True)

    try:
        clf = modelo.named_steps["clf"]
        kbest = modelo.named_steps["kbest"]
        mascara = kbest.get_support()
        colunas_selecionadas = colunas_X[mascara]

        exibiu = False

        # Importâncias nativas (ex.: RandomForest)
        if hasattr(clf, "feature_importances_"):
            importancias = pd.Series(clf.feature_importances_, index=colunas_selecionadas)
            top = importancias.sort_values(ascending=False).head(15)
            print("   Top 15 importâncias (nativas):")
            display(top)

            plt.figure()
            top.sort_values().plot(kind="barh")
            plt.title(f"Top 15 Importâncias — {nome}")
            plt.tight_layout()
            plt.savefig(pasta / f"{nome}_importancias.png", dpi=150, bbox_inches="tight")
            plt.show()
            exibiu = True

        # Coeficientes (ex.: Regressão Logística, SVM Linear)
        elif hasattr(clf, "coef_"):
            coefs = pd.Series(clf.coef_.ravel(), index=colunas_selecionadas)
            top = coefs.reindex(coefs.abs().sort_values(ascending=False).index).head(15)
            print("   Top 15 coeficientes (|coef|):")
            display(top)

            plt.figure()
            top.sort_values().plot(kind="barh")
            plt.title(f"Top 15 Coeficientes (abs) — {nome}")
            plt.tight_layout()
            plt.savefig(pasta / f"{nome}_coeficientes.png", dpi=150, bbox_inches="tight")
            plt.show()
            exibiu = True

        # Fallback: importância por permutação (agnóstica)
        if not exibiu:
            print("   O classificador não expõe importâncias/coeficientes. Usando permutação (ROC-AUC).")
            r = permutation_importance(
                modelo, X_test, y_test,
                scoring="roc_auc", n_repeats=10, random_state=SEED, n_jobs=-1
            )
            imp = pd.DataFrame({
                "atributo": X_test.columns,
                "importancia_media": r.importances_mean,
                "importancia_std": r.importances_std
            }).sort_values("importancia_media", ascending=False).reset_index(drop=True)

            print("   Top 15 (permutação):")
            display(imp.head(15))

            plt.figure(figsize=(7, 7))
            top15 = imp.head(15).copy()
            plt.barh(top15["atributo"][::-1], top15["importancia_media"][::-1])
            plt.title(f"Top 15 por Permutação — {nome}")
            plt.xlabel("Δ ROC-AUC (média)")
            plt.tight_layout()
            plt.savefig(pasta / f"{nome}_permutacao.png", dpi=150, bbox_inches="tight")
            plt.show()

    except Exception as e:
        print("   Não foi possível extrair interpretabilidade.")
        print("   Detalhes:", e)

# Treinamento com GridSearchCV (otimizando por ROC-AUC)
print("\nIniciando treinamento")
resultados = []
melhores = {}

for nome, pipe in pipelines.items():
    print(f"Treinando {nome} ...")
    busca = GridSearchCV(
        estimator=pipe,
        param_grid=grades[nome],
        scoring="roc_auc", # métrica primária de otimização
        n_jobs=-1,
        cv=cv,
        refit=True, # refaz o ajuste final com o melhor conjunto de hiperparâmetros
        verbose=0
    )
    busca.fit(X_treino, y_treino)

    melhor = busca.best_estimator_
    melhores[nome] = melhor

    # Avaliação no TESTE (mantido oculto do treino)
    met, y_prob, y_pred = avaliar_em_teste(melhor, X_teste, y_teste)

    # Guarda o número de atributos selecionados pelo KBest
    k_selecionado = melhor.named_steps["kbest"].get_support().sum()

    resultados.append({
        "modelo": nome,
        "best_cv_roc_auc": busca.best_score_,
        "test_roc_auc": met["test_roc_auc"],
        "test_pr_auc": met["test_pr_auc"],
        "test_accuracy": met["test_accuracy"],
        "test_sensitivity": met["test_sensitivity"],   # sensibilidade (maligno)
        "test_specificity": met["test_specificity"],   # especificidade (benigno)
        "test_f1": met["test_f1"],
        "selected_k": k_selecionado,
        "best_params": busca.best_params_
    })

print("Treinamento concluído!")

# Tabela comparativa de resultados e salvamento em CSV
tabela = pd.DataFrame(resultados).sort_values(by="test_roc_auc", ascending=False).reset_index(drop=True)
tabela = tabela.set_index("modelo")

PASTA_SAIDA = Path("resultados_tcc")
PASTA_SAIDA.mkdir(exist_ok=True)

csv_path = PASTA_SAIDA / "comparacao_modelos_teste.csv"
tabela.to_csv(csv_path)

print("\nTabela comparativa (ordenada por ROC-AUC no TESTE):")
pd.set_option("display.width", 1000)
pd.set_option("display.max_colwidth", 200)
display(tabela[[
    "test_roc_auc", "test_pr_auc", "test_accuracy",
    "test_sensitivity", "test_specificity", "test_f1",
    "best_cv_roc_auc", "selected_k", "best_params"
]])

print(f"Tabela salva em: {csv_path.resolve()}")

# Gráficos e interpretabilidade do melhor modelo (no TESTE)
melhor_nome = tabela.index[0]
melhor_modelo = melhores[melhor_nome]
print(f"\nMelhor modelo pelo critério ROC-AUC: {melhor_nome}")

# Recalcular previsões e probabilidades para clareza
metricas, yprob, yhat = avaliar_em_teste(melhor_modelo, X_teste, y_teste)

print("   Métricas do melhor modelo:")
for k in ["test_roc_auc","test_pr_auc","test_accuracy","test_sensitivity","test_specificity","test_f1","tn","fp","fn","tp"]:
    print(f"   - {k}: {metricas[k]:.4f}" if isinstance(metricas[k], float) else f"   - {k}: {metricas[k]}")

# Plots e salvamento
plotar_graficos_desempenho(y_teste, yprob, yhat, melhor_nome, PASTA_SAIDA)

# Interpretabilidade (importâncias/coeficientes ou permutação)
interpretar_modelo(melhor_modelo, X_teste, y_teste, melhor_nome, PASTA_SAIDA, X.columns)

# Salvamento do pipeline campeão (.pkl) e função de inferência
PASTA_MODELOS = Path("modelos")
PASTA_MODELOS.mkdir(exist_ok=True)

caminho_modelo = PASTA_MODELOS / f"{melhor_nome}_pipeline_seed{SEED}.pkl"
joblib.dump(melhor_modelo, caminho_modelo)
print(f"\nPipeline salvo em: {caminho_modelo.resolve()}")

def inferir_um_exemplo(exemplo_dict, pipeline=melhor_modelo, colunas=X.columns):
    """
    Realiza inferência para UM exemplo (dicionário: atributo -> valor).
    Retorna:
        prob_maligno (float), classe_predita (int: 1=maligno, 0=benigno)
    Observação: a ordem/nomes das colunas precisa ser idêntica ao treinamento.
    """
    df = pd.DataFrame([exemplo_dict])[colunas]
    prob = (pipeline.predict_proba(df)[:, 1]
            if hasattr(pipeline[-1], "predict_proba")
            else pipeline.decision_function(df))
    yhat = pipeline.predict(df)
    return float(prob[0]), int(yhat[0])

# Exemplo de uso da função de inferência: usando médias do conjunto de treino
exemplo = X_treino.mean().to_dict()
p, y_ = inferir_um_exemplo(exemplo)
print(f"Exemplo de inferência (usando médias dos atributos): P(maligno)={p:.3f} ; Classe={y_} (1=maligno, 0=benigno)")

print("\nFINALIZADO")
print(f"Arquivos gerados em: {PASTA_SAIDA.resolve()}  e  {PASTA_MODELOS.resolve()}")